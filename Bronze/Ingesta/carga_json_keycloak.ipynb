{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334df89d-ee9e-4e09-938f-0f7af122cf9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lower, regexp_extract, concat, lit, split, from_json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e2ad83-9ad2-4a20-b0da-3561d4502596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient\n",
    "import os\n",
    "\n",
    "# Crear SparkSession si no existe (en Databricks ya existe `spark`)\n",
    "spark = SparkSession.builder.appName(\"JsonBlobReader\").getOrCreate()\n",
    "\n",
    "# ðŸ”‘ ConexiÃ³n a tu Storage Account (desde KeyVault / Databricks Secrets)\n",
    "connection_string = dbutils.secrets.get(scope='secret-storeview', key='connectionstring-portalbi')\n",
    "container_name = \"insights-logs-appserviceconsolelogs\"\n",
    "\n",
    "# Prefijos de rutas\n",
    "source_prefix = \"raw/json_files/\"        # Ruta de origen con AppendBlob\n",
    "target_prefix = \"processed/json_files/\"  # Ruta destino para BlockBlob\n",
    "\n",
    "# Cliente de Azure Blob\n",
    "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    "\n",
    "# ðŸš€ Convertir todos los AppendBlob a BlockBlob\n",
    "for blob in container_client.list_blobs(name_starts_with=source_prefix):\n",
    "    blob_client = container_client.get_blob_client(blob)\n",
    "\n",
    "    props = blob_client.get_blob_properties()\n",
    "    if props.blob_type == \"AppendBlob\":\n",
    "        print(f\"Convirtiendo AppendBlob â†’ BlockBlob: {blob.name}\")\n",
    "\n",
    "        # Descargar AppendBlob (como bytes)\n",
    "        data = blob_client.download_blob().readall()\n",
    "\n",
    "        # Definir el nuevo nombre manteniendo jerarquÃ­a de carpetas\n",
    "        new_name = blob.name.replace(source_prefix, target_prefix, 1)\n",
    "\n",
    "        # Subir como BlockBlob\n",
    "        container_client.upload_blob(\n",
    "            name=new_name,\n",
    "            data=data,\n",
    "            blob_type=\"BlockBlob\",\n",
    "            overwrite=True\n",
    "        )\n",
    "\n",
    "print(\"âœ… Todos los AppendBlob fueron convertidos a BlockBlob en el nuevo path\")\n",
    "\n",
    "# ðŸ”Ž Ahora sÃ­ puedes leer los JSON con PySpark\n",
    "storage_account = \"yourstorageaccount\"  # reemplaza con tu cuenta\n",
    "df = spark.read.json(\n",
    "    f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net/{target_prefix}\"\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a969c9-07f3-4b5a-a789-b84df48a18ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ConfiguraciÃ³n de conexiÃ³n JDBC\n",
    "JDBC_CONFIG = {\n",
    "    \"hostname\": \"psql-dn-keycloak-restore.postgres.database.azure.com\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"keycloak\",\n",
    "    \"username\": dbutils.secrets.get(scope='secret-storeview', key='username-keycloak-db'),\n",
    "    \"password\": dbutils.secrets.get(scope='secret-storeview', key='password-keycloak-db'),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "jdbcUrl = f\"jdbc:postgresql://{JDBC_CONFIG['hostname']}:{JDBC_CONFIG['port']}/{JDBC_CONFIG['database']}?sslmode=require\"\n",
    "connectionProperties = {\n",
    "    \"user\": JDBC_CONFIG[\"username\"],\n",
    "    \"password\": JDBC_CONFIG[\"password\"],\n",
    "    \"driver\": JDBC_CONFIG[\"driver\"]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874d570a-8035-40f3-bead-d80d1f899fc1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "revisar"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobClient\n",
    "import json\n",
    " \n",
    "# ConfiguraciÃ³n\n",
    "account_name = \"<storage_account>\"\n",
    "account_key = \"<correct_base64_encoded_storage_key>\"\n",
    "container_name = \"<container>\"\n",
    "blob_name = \"<append_blob.json>\"\n",
    " \n",
    "# Crear cliente\n",
    "url = f\"https://{account_name}.blob.core.windows.net/{container_name}/{blob_name}\"\n",
    "blob = BlobClient(account_url=f\"https://{account_name}.blob.core.windows.net\", \n",
    "                  container_name=container_name,\n",
    "                  blob_name=blob_name,\n",
    "                  credential=account_key)\n",
    " \n",
    "# Descargar\n",
    "data = blob.download_blob().readall().decode(\"utf-8\")\n",
    " \n",
    "# Pasar a Spark\n",
    "df = spark.read.json(spark.sparkContext.parallelize([data]))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e12338d3-398f-45f0-af27-c82f199d8846",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "revisar"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    " \n",
    "# ðŸ”‘ ConexiÃ³n a tu Storage Account\n",
    "connection_string = dbutils.secrets.get(scope='secret-storeview', key='connectionstring-portalbi')\n",
    "# container_name = \"insights-logs-appserviceconsolelogs\"     --- Ruta de los archivos\n",
    "\n",
    " \n",
    "# Ruta origen (los AppendBlob)\n",
    "source_prefix = \"raw/json_files/\"   # <--- tu path \"carpeta\"\n",
    "# Ruta destino (nuevo path BlockBlob)\n",
    "target_prefix = \"processed/json_files/\"\n",
    " \n",
    "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    " \n",
    "for blob in container_client.list_blobs(name_starts_with=source_prefix):\n",
    "    blob_client = container_client.get_blob_client(blob)\n",
    " \n",
    "    # Checar tipo de blob\n",
    "    props = blob_client.get_blob_properties()\n",
    "    if props.blob_type == \"AppendBlob\":\n",
    "        print(f\"Convirtiendo: {blob.name}\")\n",
    " \n",
    "        # Descargar AppendBlob\n",
    "        data = blob_client.download_blob().readall()\n",
    " \n",
    "        # Nombre destino (manteniendo subcarpetas)\n",
    "        new_name = blob.name.replace(source_prefix, target_prefix, 1)\n",
    " \n",
    "        # Subir como BlockBlob\n",
    "        container_client.upload_blob(\n",
    "            name=new_name,\n",
    "            data=data,\n",
    "            blob_type=\"BlockBlob\",\n",
    "            overwrite=True\n",
    "        )\n",
    " \n",
    "print(\"âœ… Todos los AppendBlob fueron convertidos a BlockBlob en el nuevo path\")\n",
    " \n",
    "# ðŸ”Ž Ahora sÃ­ puedes leer con PySpark como siempre:\n",
    "df = spark.read.json(f\"wasbs://{container_name}@<storage_account>.blob.core.windows.net/{target_prefix}\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06b19a2-e11f-4404-8740-ab3edebc8f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import col, concat, lit, regexp_extract, from_json\n",
    "\n",
    "DATA_PATH = \"abfss://realtimeprueba@adlsstoreview.dfs.core.windows.net/keyclocklogs/y=2025/m=08/d=12/h=02/m=00/PT1H.json\"\n",
    "# https://adlsstoreview.blob.core.windows.net/realtimeprueba/keyclocklogs/y=2025/m=08/d=12/h=02/m=00/PT1H.json\n",
    "\n",
    "MAX_RECORDS = 1000\n",
    "\n",
    "# Definimos el esquema esperado\n",
    "schema = StructType([\n",
    "    StructField(\"Activity\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# df = spark.read.schema(schema).json(f\"{DATA_PATH}/*.json\")\n",
    "\n",
    "# dbutils.fs.ls(DATA_PATH)\n",
    "# df = spark.read.schema(schema).json(f\"{DATA_PATH}/*.json\")\n",
    "# df = spark.read.json(\"{DATA_PATH}/*/*\")\n",
    "df = spark.read.schema(schema).json(DATA_PATH)\n",
    "\n",
    "\n",
    "\n",
    "df = df.filter(col(\"Activity\").isNotNull())\n",
    "\n",
    "# ConstrucciÃ³n de la cadena JSON intermedia\n",
    "df = df.withColumn(\n",
    "    \"resultDescription_listado\",\n",
    "    concat(\n",
    "        lit(\"{\"),\n",
    "        regexp_extract(col(\"Activity\"), \"\\\\{(.*?)\\\\}\", 1),\n",
    "        lit(',\"Time\":\"'), col(\"Time\"), lit('\"}')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parseo de la cadena intermedia a columnas usando el mismo esquema\n",
    "df = df.select(from_json(col(\"resultDescription_listado\"), schema).alias(\"parsed\")).select(\"parsed.*\")\n",
    "\n",
    "# Filtramos solo \"View Report\" y eliminamos duplicados\n",
    "df = df.filter(col(\"Activity\") == \"View Report\").distinct()\n",
    "\n",
    "# Mostrar solo los primeros registros\n",
    "df.limit(MAX_RECORDS).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4700e468-988f-4aeb-9795-283b773e0df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## PRUEBA DE VALIDACIÃ“N - ingesta_logs_blob\n",
    "\n",
    "# ConexiÃ³n a Azure Blob Storage\n",
    "conn_string = dbutils.secrets.get(scope='secret-storeview', key='connectionstring-portalbi')\n",
    "container_name = \"insights-logs-appserviceconsolelogs\"\n",
    "\n",
    "# Leer mÃ¡ximo 3 blobs y mÃ¡ximo 100 lÃ­neas de datos (ParÃ¡metros de control de carga)\n",
    "MAX_BLOBS = 10\n",
    "MAX_RECORDS = 1000\n",
    "\n",
    "blob_service = BlobServiceClient.from_connection_string(conn_string)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    "\n",
    "data_list = []\n",
    "blob_count = 0\n",
    "\n",
    "for blob in container_client.list_blobs():\n",
    "    if blob.name.endswith('.json'):\n",
    "        blob_count += 1\n",
    "        content = container_client.get_blob_client(blob).download_blob().content_as_text()\n",
    "        for entry in content.strip().split('\\n'):\n",
    "            if len(data_list) >= MAX_RECORDS:\n",
    "                break\n",
    "            try:\n",
    "                data_list.append(json.loads(entry))\n",
    "            except:\n",
    "                continue\n",
    "    if blob_count >= MAX_BLOBS or len(data_list) >= MAX_RECORDS:\n",
    "        break\n",
    "\n",
    "# Esquema y transformaciÃ³n\n",
    "schema = StructType([\n",
    "    StructField(\"Activity\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data_list)\n",
    "df = df.filter(\"resultDescription like '%Activity%'\")\n",
    "df = df.withColumn(\"resultDescription_listado\", concat(\n",
    "    lit(\"{\"),\n",
    "    regexp_extract(col(\"resultDescription\"), \"\\\\{(.*?)\\\\}\", 1),\n",
    "    lit(',\"Time\":\"'), col(\"time\"), lit('\"}')\n",
    "))\n",
    "df = df.select(from_json(col(\"resultDescription_listado\"), schema).alias(\"parsed\")).select(\"parsed.*\")\n",
    "df = df.filter(col(\"Activity\") == \"View Report\").distinct()\n",
    "\n",
    "# Mostrar una muestra (sin cargar todo)\n",
    "df.limit(100).display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1ed6f3-b5ad-45cd-add2-d2ca568f49bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar en delta\n",
    "\n",
    "path_delta = \"/mnt/bronze/delta/insights_logs_appserviceconsolelogs\"\n",
    "df.write.format(\"delta\").mode(\"append\").save(path_delta)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS bronze.insights_logs_appserviceconsolelogs USING DELTA LOCATION '{path_delta}/'\")\n",
    "spark.sql(\"REFRESH TABLE bronze.insights_logs_appserviceconsolelogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a842c0-7ffa-4271-b257-fb2eabbc04e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "consulta = spark.sql(\"\"\"select * from bronze.insights_logs_appserviceconsolelogs\"\"\")\n",
    "display(consulta)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4970882964299067,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "carga_json_keycloak",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
