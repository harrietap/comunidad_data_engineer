{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f48e4a-1996-4cc3-beeb-c3c2bd9796f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Autor: Jairo Cifuentes\n",
    "# Fecha: 06/08/2025\n",
    "# Descripción: Carga de datos dn_user en Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272e47d3-58c9-4f8c-ab85-08268790b3af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"configID\",\"1\")\n",
    "configID = dbutils.widgets.get(\"configID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a05275-8d53-484b-b892-f6c5043c287c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configuration =  spark.sql(f\"select cliente,fuente,ruta_archivo,formato,tabla_destino,frecuencia,activo,\\\n",
    "                             ultima_ejecucion,schema from hive_metastore.bronze.tbl_config_ingesta \\\n",
    "                             where id ='{configID}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c1550e-f985-49df-a6ae-cee2955893e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ruta_archivo = configuration.collect()[0][\"ruta_archivo\"]\n",
    "fuente = configuration.collect()[0][\"fuente\"]\n",
    "formato = configuration.collect()[0][\"formato\"]\n",
    "tabla_destino = configuration.collect()[0][\"tabla_destino\"]\n",
    "frecuencia = configuration.collect()[0][\"frecuencia\"]\n",
    "schema = configuration.collect()[0][\"schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf767ec-93ec-4ee9-a0ce-df904384cabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12efe54a-b44e-4abf-be17-25ea2ab1a1d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/*update \n",
    "hive_metastore.bronze.tbl_config_ingesta\n",
    "set ruta_archivo ='{\\\"scope\\\":\\\"secret-storeview\\\", \\\"username\\\":\\\"username-keycloak-db\\\", \\\"password\\\":\\\"password-keycloak-db\\\", \\\"database\\\":\\\"keycloak\\\",\\\"hostname\\\": \\\"psql-dn-keycloak-restore-2.postgres.database.azure.com\\\", \\\"port\\\":5432, \\\"dbtable\\\":\\\"dn_user\\\"  }'*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023a1f16-a725-491a-98a7-08e2d3acc8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = json.loads(ruta_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d9bafa-7001-47f9-96ad-a6e3a42635ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config[\"scope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a969c9-07f3-4b5a-a789-b84df48a18ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Configuración de conexión JDBC\n",
    "JDBC_CONFIG = {\n",
    "    \"hostname\": config[\"hostname\"],\n",
    "    \"port\": config[\"port\"],\n",
    "    \"database\": config[\"database\"],\n",
    "    \"username\": dbutils.secrets.get(scope=config[\"scope\"], key=config[\"username\"]),\n",
    "    \"password\": dbutils.secrets.get(scope=config[\"scope\"], key=config[\"password\"]),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "jdbcUrl = f\"jdbc:postgresql://{JDBC_CONFIG['hostname']}:{JDBC_CONFIG['port']}/{JDBC_CONFIG['database']}?sslmode=require\"\n",
    "connectionProperties = {\n",
    "    \"user\": JDBC_CONFIG[\"username\"],\n",
    "    \"password\": JDBC_CONFIG[\"password\"],\n",
    "    \"driver\": JDBC_CONFIG[\"driver\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cad03ba-aab5-4974-b4da-d63604a011ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encode_schema(schema):\n",
    "    \"\"\"\n",
    "    Encodes a StructType schema into a string that can be passed as a parameter.\n",
    "    \n",
    "    Args:\n",
    "        schema (StructType): The schema to encode.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string representing the encoded schema.\n",
    "    \"\"\"\n",
    "    schema_str = \"\"\n",
    "    for field in schema:\n",
    "        field_type = field.dataType.simpleString()\n",
    "        nullable = field.nullable\n",
    "        schema_str += f\"{field.name},{field_type},{nullable}\\n\"\n",
    "    return schema_str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe54c87-4fb3-43c5-b73e-fcc3608bbf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, DecimalType,\n",
    "    DateType, DoubleType, FloatType, BooleanType, ShortType, BinaryType, TimestampType\n",
    ")\n",
    "def decode_schema(schema_str):\n",
    "    \"\"\"\n",
    "    Decodes a string into a StructType schema.\n",
    "    \n",
    "    Args:\n",
    "        schema_str (str): The string representing the encoded schema.\n",
    "    \n",
    "    Returns:\n",
    "        StructType: The decoded schema.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If there's an error processing a schema line.\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    lines = schema_str.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        try:\n",
    "            field_name, rest = line.split(',', 1)\n",
    "            \n",
    "            if rest.startswith('decimal('):\n",
    "                decimal_part, nullable = rest.rsplit(',', 1)\n",
    "                field_type = decimal_part\n",
    "                nullable = nullable.lower() == 'true'\n",
    "                precision, scale = map(int, decimal_part[8:-1].split(','))\n",
    "                data_type = DecimalType(precision, scale)\n",
    "            else:\n",
    "                field_type, nullable = rest.rsplit(',', 1)\n",
    "                nullable = nullable.lower() == 'true'\n",
    "                \n",
    "                if field_type == \"string\":\n",
    "                    data_type = StringType()\n",
    "                elif field_type in [\"int\", \"integer\"]:\n",
    "                    data_type = IntegerType()\n",
    "                elif field_type in [\"long\", \"bigint\"]:\n",
    "                    data_type = LongType()\n",
    "                elif field_type == \"double\":\n",
    "                    data_type = DoubleType()\n",
    "                elif field_type == \"float\":\n",
    "                    data_type = FloatType()\n",
    "                elif field_type == \"boolean\":\n",
    "                    data_type = BooleanType()\n",
    "                elif field_type == \"short\":\n",
    "                    data_type = ShortType()\n",
    "                elif field_type == \"binary\":\n",
    "                    data_type = BinaryType()\n",
    "                elif field_type == \"timestamp\":\n",
    "                    data_type = TimestampType()\n",
    "                elif field_type == \"date\":\n",
    "                    data_type = DateType()\n",
    "                else:\n",
    "                    raise ValueError(f\"Tipo de dato no soportado: {field_type}\")\n",
    "            \n",
    "            fields.append(StructField(field_name, data_type, nullable))\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error al procesar la línea '{line}': {str(e)}\")\n",
    "    \n",
    "    return StructType(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c489295-0d0d-4aab-bcf5-d170d1ebefcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Se importa los tipos de datos necesarios para definir el esquema del DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "# Controlar los tipos de datos y evitar inferencias automáticas de Spark\n",
    "schema_dn_user = decode_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649de8c4-4d5f-4888-a72e-97b43eaf44ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_dn_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a0ea86-e087-4a15-b146-c7d9fdadad4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"spark.sql(f\"insert into hive_metastore.bronze.tbl_config_ingesta \\\n",
    "    select 1, 'keycloak', 'postgres', '', 'sql',\\\n",
    "    'bronze.dn_user', 'diaria', 1, current_timestamp(), '{str_schema}'\")\"\"\"\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363b73d1-2ce3-4dfc-9603-eeb50bf1bde5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer la tabla \"dn_user\" desde PostgreSQL usando el esquema definido\n",
    "# Para garantizar que los tipos de datos sean los correctos\n",
    "df_dn_user = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"dbtable\", config[\"dbtable\"]) \\\n",
    "    .option(\"user\", connectionProperties[\"user\"]) \\\n",
    "    .option(\"password\", connectionProperties[\"password\"]) \\\n",
    "    .option(\"driver\", connectionProperties[\"driver\"]) \\\n",
    "    .schema(schema_dn_user) \\\n",
    "    .load()\n",
    "\n",
    "# Mostrar los primeros 10 registros para verificar la lectura\n",
    "display(df_dn_user.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1b0f9a-1071-46ed-8dbc-a1d4a9141789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validar el esquema\n",
    "def validar_esquema_postgresql(jdbc_url, table_name, connection_props, schema_dn_user):\n",
    "\n",
    "    # Leer la tabla desde PostgreSQL usando el esquema definido\n",
    "    df_actual = spark.read.jdbc(url=jdbc_url, table=table_name, properties=connection_props)\n",
    "    actual_schema = df_actual.schema\n",
    "\n",
    "    # Comparar los esquemas\n",
    "    expected_fields = {field.name: type(field.dataType) for field in schema_dn_user.fields}\n",
    "    actual_fields = {field.name: type(field.dataType) for field in actual_schema.fields}\n",
    "\n",
    "    errores = False # Variable para controlar si hay errores\n",
    "\n",
    "    #Revisar si hay colummnas faltantes o con tipos diferentes\n",
    "    for col in expected_fields:\n",
    "        if col not in actual_fields:\n",
    "            print(f\"⚠️ Columna faltante: '{col}'\")\n",
    "            errores = True\n",
    "        elif expected_fields[col] != actual_fields[col]:\n",
    "            print(f\"⚠️ Tipo cambiado en '{col}': esperado {expected_fields[col]}, recibido {actual_fields[col]}\")\n",
    "            errores = True\n",
    "\n",
    "    #Revisar si hay columnas adicionales\n",
    "    for col in actual_fields:\n",
    "        if col not in expected_fields:\n",
    "            print(f\"⚠️ Columna adicional no esperada: '{col}'\")\n",
    "            errores = True\n",
    "\n",
    "    # Mostrar el resdultado de si hubo o no errores\n",
    "    if errores:\n",
    "        print(\"❌ El esquema actual NO coincide con el esperado.\")\n",
    "    else:\n",
    "        print(\"✅ El esquema actual coincide con el esperado.\")\n",
    "\n",
    "        # Mostrar los primeros 10 registros para verificar la lectura\n",
    "        display(df_actual.limit(10))\n",
    "\n",
    "# Ejecutar la validación\n",
    "validar_esquema_postgresql(jdbcUrl, \"dn_user\", connectionProperties, schema_dn_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0013903-55ae-4963-b1fc-8ecda275ea8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Escribir en delta\n",
    "df_dn_user.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tabla_destino)\n",
    "spark.sql(f\"REFRESH TABLE {tabla_destino}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7516750187062058,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingesta_delta",
   "widgets": {
    "configID": {
     "currentValue": "1",
     "nuid": "4112184b-af3c-4ac3-a8c2-304347271ad1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1",
      "label": null,
      "name": "configID",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1",
      "label": null,
      "name": "configID",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
