{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f48e4a-1996-4cc3-beeb-c3c2bd9796f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Autor: Jairo Cifuentes\n",
    "# Fecha: 06/08/2025\n",
    "# Descripción: Carga de datos dn_user en Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a969c9-07f3-4b5a-a789-b84df48a18ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Configuración de conexión JDBC\n",
    "JDBC_CONFIG = {\n",
    "    \"hostname\": \"psql-dn-keycloak-restore-2.postgres.database.azure.com\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"keycloak\",\n",
    "    \"username\": dbutils.secrets.get(scope='secret-storeview', key='username-keycloak-db'),\n",
    "    \"password\": dbutils.secrets.get(scope='secret-storeview', key='password-keycloak-db'),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "jdbcUrl = f\"jdbc:postgresql://{JDBC_CONFIG['hostname']}:{JDBC_CONFIG['port']}/{JDBC_CONFIG['database']}?sslmode=require\"\n",
    "connectionProperties = {\n",
    "    \"user\": JDBC_CONFIG[\"username\"],\n",
    "    \"password\": JDBC_CONFIG[\"password\"],\n",
    "    \"driver\": JDBC_CONFIG[\"driver\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c489295-0d0d-4aab-bcf5-d170d1ebefcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Se importa los tipos de datos necesarios para definir el esquema del DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "# Controlar los tipos de datos y evitar inferencias automáticas de Spark\n",
    "schema_dn_user = StructType([\n",
    "    StructField(\"parent_group\", StringType(), True),\n",
    "    StructField(\"grupo\", StringType(), True),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"apellido\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"enabled\", BooleanType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363b73d1-2ce3-4dfc-9603-eeb50bf1bde5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer la tabla \"dn_user\" desde PostgreSQL usando el esquema definido\n",
    "# Para garantizar que los tipos de datos sean los correctos\n",
    "df_dn_user = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"dbtable\", \"dn_user\") \\\n",
    "    .option(\"user\", connectionProperties[\"user\"]) \\\n",
    "    .option(\"password\", connectionProperties[\"password\"]) \\\n",
    "    .option(\"driver\", connectionProperties[\"driver\"]) \\\n",
    "    .schema(schema_dn_user) \\\n",
    "    .load()\n",
    "\n",
    "# Mostrar los primeros 10 registros para verificar la lectura\n",
    "display(df_dn_user.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1b0f9a-1071-46ed-8dbc-a1d4a9141789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validar el esquema\n",
    "def validar_esquema_postgresql(jdbc_url, table_name, connection_props, schema_dn_user):\n",
    "\n",
    "    # Leer la tabla desde PostgreSQL usando el esquema definido\n",
    "    df_actual = spark.read.jdbc(url=jdbc_url, table=table_name, properties=connection_props)\n",
    "    actual_schema = df_actual.schema\n",
    "\n",
    "    # Comparar los esquemas\n",
    "    expected_fields = {field.name: type(field.dataType) for field in schema_dn_user.fields}\n",
    "    actual_fields = {field.name: type(field.dataType) for field in actual_schema.fields}\n",
    "\n",
    "    errores = False # Variable para controlar si hay errores\n",
    "\n",
    "    #Revisar si hay colummnas faltantes o con tipos diferentes\n",
    "    for col in expected_fields:\n",
    "        if col not in actual_fields:\n",
    "            print(f\"⚠️ Columna faltante: '{col}'\")\n",
    "            errores = True\n",
    "        elif expected_fields[col] != actual_fields[col]:\n",
    "            print(f\"⚠️ Tipo cambiado en '{col}': esperado {expected_fields[col]}, recibido {actual_fields[col]}\")\n",
    "            errores = True\n",
    "\n",
    "    #Revisar si hay columnas adicionales\n",
    "    for col in actual_fields:\n",
    "        if col not in expected_fields:\n",
    "            print(f\"⚠️ Columna adicional no esperada: '{col}'\")\n",
    "            errores = True\n",
    "\n",
    "    # Mostrar el resdultado de si hubo o no errores\n",
    "    if errores:\n",
    "        print(\"❌ El esquema actual NO coincide con el esperado.\")\n",
    "    else:\n",
    "        print(\"✅ El esquema actual coincide con el esperado.\")\n",
    "\n",
    "        # Mostrar los primeros 10 registros para verificar la lectura\n",
    "        display(df_actual.limit(10))\n",
    "\n",
    "# Ejecutar la validación\n",
    "validar_esquema_postgresql(jdbcUrl, \"dn_user\", connectionProperties, schema_dn_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4d0e76-a391-46e5-bdb6-9883e2136d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Leer objeto desde PostgreSQL (quitar si es necesario ya que ahora se hace con schema)\n",
    "df_dn_user = spark.read.jdbc(url=jdbcUrl, table=\"dn_user\", properties=connectionProperties)\n",
    "display(df_dn_user.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0013903-55ae-4963-b1fc-8ecda275ea8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ruta destino en Bronze\n",
    "bronze_path = \"/mnt/bronze/keycloak/dn_user\"\n",
    "\n",
    "# Eliminar en caso que exista\n",
    "try:\n",
    "  dbutils.fs.ls(bronze_path)\n",
    "  print(\"Existe, se sobreescribirá\")\n",
    "except:\n",
    "  print(\"No existe, se creará\")\n",
    "\n",
    "# Escribir en delta\n",
    "df_dn_user.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
    "\n",
    "# Registrar tabla\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze.bronze_dn_user\n",
    "    USING DELTA\n",
    "    LOCATION '{bronze_path}/'\n",
    "\"\"\")\n",
    "spark.sql(\"REFRESH TABLE bronze.bronze_dn_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce3bfb7-a038-48d3-a014-36e261455923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificar lectura\n",
    "df_val = spark.table(\"bronze.bronze_dn_user\")\n",
    "display(df_val.limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4970882964299067,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingesta_dn_user_delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
