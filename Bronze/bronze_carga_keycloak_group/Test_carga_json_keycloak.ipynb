{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334df89d-ee9e-4e09-938f-0f7af122cf9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lower, regexp_extract, concat, lit, split, from_json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a969c9-07f3-4b5a-a789-b84df48a18ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Configuración de conexión JDBC\n",
    "JDBC_CONFIG = {\n",
    "    \"hostname\": \"psql-dn-keycloak-restore.postgres.database.azure.com\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"keycloak\",\n",
    "    \"username\": dbutils.secrets.get(scope='secret-storeview', key='username-keycloak-db'),\n",
    "    \"password\": dbutils.secrets.get(scope='secret-storeview', key='password-keycloak-db'),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "jdbcUrl = f\"jdbc:postgresql://{JDBC_CONFIG['hostname']}:{JDBC_CONFIG['port']}/{JDBC_CONFIG['database']}?sslmode=require\"\n",
    "connectionProperties = {\n",
    "    \"user\": JDBC_CONFIG[\"username\"],\n",
    "    \"password\": JDBC_CONFIG[\"password\"],\n",
    "    \"driver\": JDBC_CONFIG[\"driver\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26815730-b2f3-4b3f-b0ca-cc650af8da69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).json(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06b19a2-e11f-4404-8740-ab3edebc8f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import col, concat, lit, regexp_extract, from_json\n",
    "\n",
    "DATA_PATH = \"/mnt/adlsstoreview/realtimeprueba/keyclocklogs/y=2025/m=08/d=01/h=00/m=00/*json\"\n",
    "# https://adlsstoreview.blob.core.windows.net/realtimeprueba/keyclocklogs/y=2025/m=08/d=12/h=02/m=00/PT1H.json\n",
    "\n",
    "MAX_RECORDS = 1000\n",
    "\n",
    "# Definimos el esquema esperado\n",
    "schema = StructType([\n",
    "    StructField(\"Activity\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# df = spark.read.schema(schema).json(f\"{DATA_PATH}/*.json\")\n",
    "\n",
    "# dbutils.fs.ls(DATA_PATH)\n",
    "# df = spark.read.schema(schema).json(f\"{DATA_PATH}/*.json\")\n",
    "# df = spark.read.json(\"{DATA_PATH}/*/*\")\n",
    "df = spark.read.schema(schema).json(DATA_PATH)\n",
    "\n",
    "\n",
    "\n",
    "df = df.filter(col(\"Activity\").isNotNull())\n",
    "\n",
    "# Construcción de la cadena JSON intermedia\n",
    "df = df.withColumn(\n",
    "    \"resultDescription_listado\",\n",
    "    concat(\n",
    "        lit(\"{\"),\n",
    "        regexp_extract(col(\"Activity\"), \"\\\\{(.*?)\\\\}\", 1),\n",
    "        lit(',\"Time\":\"'), col(\"Time\"), lit('\"}')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Parseo de la cadena intermedia a columnas usando el mismo esquema\n",
    "df = df.select(from_json(col(\"resultDescription_listado\"), schema).alias(\"parsed\")).select(\"parsed.*\")\n",
    "\n",
    "# Filtramos solo \"View Report\" y eliminamos duplicados\n",
    "df = df.filter(col(\"Activity\") == \"View Report\").distinct()\n",
    "\n",
    "# Mostrar solo los primeros registros\n",
    "df.limit(MAX_RECORDS).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7399e9e1-650d-4620-8946-296fefa71f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/mnt/adlsstoreview/realtimeprueba/keyclocklogs/y=2025/m=08/d=01/h=16/m=00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0087ccba-2bfd-441b-be56-e4d5def47fcd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755882912073}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"resultDescription\", StringType(), True),\n",
    "    StructField(\"resourceId\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "    StructField(\"operationName\", StringType(), True),\n",
    "    StructField(\"containerId\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"EventStampType\", StringType(), True),\n",
    "    StructField(\"EventPrimaryStampName\", StringType(), True),\n",
    "    StructField(\"EventStampName\", StringType(), True),\n",
    "    StructField(\"Host\", StringType(), True),\n",
    "    StructField(\"EventIpAddress\", StringType(), True)\n",
    "])\n",
    "\n",
    "path = (\n",
    "    \"wasbs://realtimeprueba@adlsstoreview.blob.core.windows.net/\"\n",
    "    \"keyclocklogs/y=2025/m=08/d=12/h=02/m=00/\"\n",
    ")\n",
    "df = spark.read.option(\"recursiveFileLookup\", \"true\").schema(schema).json(path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99ed1ea-79c4-4c25-b9a4-8bd228e8092d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_log_path = \"abfss://realtimeprueba@adlsstoreview.dfs.core.windows.net/keyclocklogs/y=2025/m=08/d=12/h=02/m=00/PT1H.json\"\n",
    "\n",
    "# Read the JSON data\n",
    "spark.conf.set(\"fs.azure.account.key.adlsstoreview.dfs.core.windows.net\", \"<YOUR_STORAGE_ACCOUNT_KEY>\")\n",
    "df = spark.read.json(json_log_path, multiLine=True)\n",
    "\n",
    "\n",
    "# Display the schema and some data\n",
    "df.printSchema()\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fd6aaa2-0909-40f6-b868-08e16481e418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4700e468-988f-4aeb-9795-283b773e0df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## PRUEBA DE VALIDACIÓN - ingesta_logs_blob\n",
    "\n",
    "# Conexión a Azure Blob Storage\n",
    "conn_string = dbutils.secrets.get(scope='secret-storeview', key='connectionstring-portalbi')\n",
    "container_name = \"insights-logs-appserviceconsolelogs\"\n",
    "\n",
    "# Leer máximo 3 blobs y máximo 100 líneas de datos (Parámetros de control de carga)\n",
    "MAX_BLOBS = 10\n",
    "MAX_RECORDS = 1000\n",
    "\n",
    "blob_service = BlobServiceClient.from_connection_string(conn_string)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    "\n",
    "data_list = []\n",
    "blob_count = 0\n",
    "\n",
    "for blob in container_client.list_blobs():\n",
    "    if blob.name.endswith('.json'):\n",
    "        blob_count += 1\n",
    "        content = container_client.get_blob_client(blob).download_blob().content_as_text()\n",
    "        for entry in content.strip().split('\\n'):\n",
    "            if len(data_list) >= MAX_RECORDS:\n",
    "                break\n",
    "            try:\n",
    "                data_list.append(json.loads(entry))\n",
    "            except:\n",
    "                continue\n",
    "    if blob_count >= MAX_BLOBS or len(data_list) >= MAX_RECORDS:\n",
    "        break\n",
    "\n",
    "# Esquema y transformación\n",
    "schema = StructType([\n",
    "    StructField(\"Activity\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data_list)\n",
    "df = df.filter(\"resultDescription like '%Activity%'\")\n",
    "df = df.withColumn(\"resultDescription_listado\", concat(\n",
    "    lit(\"{\"),\n",
    "    regexp_extract(col(\"resultDescription\"), \"\\\\{(.*?)\\\\}\", 1),\n",
    "    lit(',\"Time\":\"'), col(\"time\"), lit('\"}')\n",
    "))\n",
    "df = df.select(from_json(col(\"resultDescription_listado\"), schema).alias(\"parsed\")).select(\"parsed.*\")\n",
    "df = df.filter(col(\"Activity\") == \"View Report\").distinct()\n",
    "\n",
    "# Mostrar una muestra (sin cargar todo)\n",
    "df.limit(100).display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1ed6f3-b5ad-45cd-add2-d2ca568f49bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar en delta\n",
    "\n",
    "path_delta = \"/mnt/bronze/delta/insights_logs_appserviceconsolelogs\"\n",
    "df.write.format(\"delta\").mode(\"append\").save(path_delta)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS bronze.insights_logs_appserviceconsolelogs USING DELTA LOCATION '{path_delta}/'\")\n",
    "spark.sql(\"REFRESH TABLE bronze.insights_logs_appserviceconsolelogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a842c0-7ffa-4271-b257-fb2eabbc04e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "consulta = spark.sql(\"\"\"select * from bronze.insights_logs_appserviceconsolelogs\"\"\")\n",
    "display(consulta)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4970882964299067,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Test_carga_json_keycloak",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
