{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334df89d-ee9e-4e09-938f-0f7af122cf9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lower, regexp_extract, concat, lit, split, from_json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20201797-3246-4077-95e5-d4ba30e181f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Configuración de conexión JDBC\n",
    "JDBC_CONFIG = {\n",
    "    \"hostname\": \"psql-dn-keycloak-restore.postgres.database.azure.com\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"keycloak\",\n",
    "    \"username\": dbutils.secrets.get(scope='secret-storeview', key='username-keycloak-db'),\n",
    "    \"password\": dbutils.secrets.get(scope='secret-storeview', key='password-keycloak-db'),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "jdbcUrl = f\"jdbc:postgresql://{JDBC_CONFIG['hostname']}:{JDBC_CONFIG['port']}/{JDBC_CONFIG['database']}?sslmode=require\"\n",
    "connectionProperties = {\n",
    "    \"user\": JDBC_CONFIG[\"username\"],\n",
    "    \"password\": JDBC_CONFIG[\"password\"],\n",
    "    \"driver\": JDBC_CONFIG[\"driver\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2e1f06-6c64-4143-a32f-16343567fae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Crear SparkSession si no existe\n",
    "spark = SparkSession.builder.appName(\"JsonBlobReader\").getOrCreate()\n",
    "\n",
    "# Conexión a tu Storage Account\n",
    "connection_string = \"BlobEndpoint=https://adlsstoreview.blob.core.windows.net/;QueueEndpoint=https://adlsstoreview.queue.core.windows.net/;FileEndpoint=https://adlsstoreview.file.core.windows.net/;TableEndpoint=https://adlsstoreview.table.core.windows.net/;SharedAccessSignature=sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-10-09T01:33:59Z&st=2025-08-28T17:18:59Z&spr=https&sig=6M5CEY455HOFCu7R2V2T5TOOaW6GKNg%2FT5jhb5GJAaI%3D\"\n",
    "container_name = \"realtimeprueba\"\n",
    "storage_account = \"adlsstoreview\"\n",
    "\n",
    "# Paths relativos dentro del contenedor (OJO: no usar /mnt aquí)\n",
    "source_prefix = \"PT1H\"\n",
    "target_prefix = \"PT1H_block\"\n",
    "# print(connection_string)\n",
    "# Cliente de Azure Blob\n",
    "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    "\n",
    "# Conversión AppendBlob → BlockBlob\n",
    "for blob in container_client.list_blobs():\n",
    "    blob_client = container_client.get_blob_client(blob)\n",
    "    props = blob_client.get_blob_properties()\n",
    "    if props.blob_type == \"AppendBlob\":\n",
    "        print(f\"Convirtiendo AppendBlob → BlockBlob: {blob.name}\")\n",
    "        data = blob_client.download_blob().readall()\n",
    "\n",
    "        # Forzar extensión .json para que Spark lo pueda leer\n",
    "        new_name = blob.name.replace(source_prefix, target_prefix, 1)\n",
    "        if not new_name.endswith(\".json\"):\n",
    "            new_name = new_name + \".json\"\n",
    "\n",
    "        container_client.upload_blob(\n",
    "            name=new_name,\n",
    "            data=data,\n",
    "            blob_type=\"BlockBlob\",\n",
    "            overwrite=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474c9f69-1491-4caa-ae0a-d1fed04f7f44",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"resultDescription\":983},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757533607334}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"time\", StringType(), True),  # o TimestampType() si quieres convertirlo\n",
    "    StructField(\"resultDescription\", StringType(), True),\n",
    "    StructField(\"resourceId\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "    StructField(\"operationName\", StringType(), True),\n",
    "    StructField(\"containerId\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"EventStampType\", StringType(), True),\n",
    "    StructField(\"EventPrimaryStampName\", StringType(), True),\n",
    "    StructField(\"EventStampName\", StringType(), True),\n",
    "    StructField(\"Host\", StringType(), True),\n",
    "    StructField(\"EventIpAddress\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read.json(\"/mnt/adlsstoreview/realtimeprueba/keyclocklogs/y=2025/m=08/d=01/h=00/m=00/PT1H_block.json\")\n",
    "\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccec396e-25f1-4cd3-8266-7a2a11f94281",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757533754417}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import regexp_extract, from_json, col\n",
    "\n",
    "inner_schema = StructType([\n",
    "    StructField(\"Activity\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"DurationInSecond\", DoubleType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Extraer solo la parte JSON dentro de resultDescription\n",
    "df_clean = df.withColumn(\n",
    "    \"result_json\",\n",
    "    regexp_extract(col(\"resultDescription\"), r\"(\\{.*\\})\", 1)\n",
    ")\n",
    "\n",
    "\n",
    "# Parsear el JSON y expandir columnas\n",
    "df_parsed = df_clean.withColumn(\n",
    "    \"parsed\",\n",
    "    from_json(col(\"result_json\"), inner_schema)\n",
    ").select(\n",
    "    \"*\",\n",
    "    col(\"parsed.Activity\").alias(\"Activity\"),\n",
    "    col(\"parsed.Email\").alias(\"Email\"),\n",
    "    col(\"parsed.Time\").alias(\"InnerTime\"),\n",
    "    col(\"parsed.DurationInSecond\").alias(\"DurationInSecond\"),\n",
    "    col(\"parsed.ItemId\").alias(\"ItemId\")\n",
    ").drop(\"parsed\", \"result_json\")\n",
    "\n",
    "display(df_parsed.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2f66dc-d24e-4371-b726-e1fd3f376e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar en delta\n",
    "\n",
    "path_delta = \"/mnt/calidad_datos/Bronze/Ingesta/delta/carga_json_keycloak\"\n",
    "df_parsed.write.format(\"delta\").mode(\"overwrite\").save(path_delta)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS bronze.carga_json_keycloak USING DELTA LOCATION '{path_delta}/'\")\n",
    "spark.sql(\"REFRESH TABLE bronze.carga_json_keycloak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f76b455-0349-4ef7-8e1f-33fdd4f8581b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"containerId\":256,\"Activity\":158,\"#row_number#\":52,\"time\":196},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757020090560}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "consulta = spark.sql(\"\"\"select * from bronze.carga_json_keycloak\"\"\")\n",
    "display(consulta)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4970882964299067,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_carga_json_keycloak",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
